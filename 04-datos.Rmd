# Trabajando con datos {#datos}

## Objetivos del capítulo

Al finalizar este capítulo, serás capaz de:

- Distinguir entre tipos de datos según estructura y fuente
- Evaluar calidad de datos y detectar problemas comunes
- Navegar el entorno de R y RStudio
- Importar datos desde múltiples formatos
- Realizar operaciones básicas de manipulación de datos en R
- Exportar resultados para análisis posterior o publicación

## Tipos de datos

Los datos que usamos en ciencias políticas varían en estructura, granularidad y fuente. Entender estas diferencias es crucial para seleccionar estrategias analíticas apropiadas.

### Según estructura

**Datos de corte transversal (cross-sectional)**

Observaciones de múltiples unidades en un solo momento. Ejemplo: encuesta a 1,500 votantes en octubre 2024. Cada fila es un individuo; columnas son variables (edad, educación, intención de voto).

Ventaja: Simplicidad analítica. 
Limitación: No observamos cambio temporal. No podemos distinguir efectos de período de efectos de cohorte.

**Datos de series temporales (time series)**

Observaciones de una unidad a través del tiempo. Ejemplo: tasa de desempleo mensual en Chile 2000-2024. Cada fila es un mes; columnas son variables económicas.

Ventaja: Captura dinámica temporal.
Limitación: Una sola unidad limita generalización. Difícil controlar confusores que varían en el tiempo.

**Datos de panel (longitudinal)**

Observaciones de múltiples unidades a través del tiempo. Ejemplo: datos electorales de 346 comunas chilenas en 8 elecciones presidenciales. Cada fila es comuna-año; columnas son variables.

Ventaja: Controla diferencias persistentes entre unidades (efectos fijos). Observa cambios.
Limitación: Complejidad analítica. Requiere supuestos sobre errores correlacionados.

**Datos jerárquicos (nested/multilevel)**

Observaciones anidadas en estructuras de múltiples niveles. Ejemplo: estudiantes (nivel 1) dentro de escuelas (nivel 2) dentro de comunas (nivel 3).

Ventaja: Modela variación en múltiples niveles simultáneamente.
Limitación: Requiere técnicas multinivel (más allá del alcance de este libro introductorio).

### Según granularidad

**Datos micro (individuales)**

La unidad de observación es el individuo: persona, votante, legislador, manifestante. Estos datos permiten analizar comportamiento individual y heterogeneidad.

**Datos meso (organizacionales/locales)**

La unidad es un agregado intermedio: municipios, distritos electorales, partidos políticos, organizaciones. Balance entre detalle y manejo de volumen.

**Datos macro (nacionales/internacionales)**

La unidad es el país o la región. Permiten comparación internacional pero pierden variación subnacional. Además, típicamente N es pequeño (196 países en el mundo).

La elección de granularidad no es neutral. @king1997 advierten sobre la **falacia ecológica**: inferir comportamiento individual desde patrones agregados puede ser erróneo si composición de agregados varía.

### Según fuente

**Datos primarios**

Recolectados por el investigador para el proyecto específico. Encuestas propias, experimentos, observación directa, entrevistas codificadas. 

Ventaja: Control total sobre qué se mide y cómo.
Limitación: Costoso en tiempo y recursos.

**Datos secundarios**

Recolectados por otros, usados por el investigador. Censos, estadísticas oficiales, encuestas de opinión pública, datos administrativos.

Ventaja: Eficiencia. Acceso a muestras grandes y representativas.
Limitación: No se diseñaron para nuestras preguntas específicas. Debemos aceptar decisiones de medición ajenas.

**Datos administrativos**

Generados por procesos burocráticos: registros de votantes, datos tributarios, registros judiciales, historiales legislativos.

Ventaja: Cobertura completa (no muestral), alta confiabilidad.
Limitación: Reflejan categorías burocráticas, no conceptos teóricos. Problemas de acceso y privacidad.

**Datos digitales**

Generados por actividad en línea: tweets, posts de Facebook, búsquedas en Google, patrones de navegación. Creciente uso en ciencias sociales computacionales.

Ventaja: Volumen masivo, observación de comportamiento real (no reportado).
Limitación: Representatividad problemática (no todos están en línea). Inferir actitudes desde comportamiento digital es complicado.

## Fuentes de datos para ciencias políticas

### Encuestas de opinión pública

**Internacionales:**
- **World Values Survey**: Actitudes y valores en ~100 países
- **Latinobarómetro**: Opinión pública en 18 países latinoamericanos anualmente
- **AmericasBarometer (LAPOP)**: Actitudes políticas en las Américas

**Chile:**
- **Encuesta CEP**: Trimestral desde 1987, serie temporal más larga en Chile
- **CASEN**: Caracterización socioeconómica nacional (bienal)
- **Encuesta Bicentenario UC-Adimark**

### Datos electorales

- **Servicio Electoral de Chile (SERVEL)**: Resultados electorales desagregados
- **International IDEA**: Datos electorales comparados internacionalmente
- **Database of Political Institutions (DPI)**: Instituciones y resultados electorales

### Datos económicos y sociales

- **Banco Mundial**: Indicadores de desarrollo mundial
- **CEPAL**: Estadísticas económicas y sociales de América Latina
- **INE Chile**: Estadísticas oficiales nacionales
- **OECD**: Datos comparados de países desarrollados

### Datos sobre instituciones políticas

- **Polity V**: Características de regímenes políticos
- **V-Dem**: Variedades de democracia, múltiples dimensiones
- **Database of Political Institutions (DPI)**: Sistemas electorales, estructuras gubernamentales
- **Comparative Constitutions Project**: Contenido de constituciones

### Datos legislativos

- **VoteView**: Votaciones nominales del Congreso de EE.UU.
- **Manifesto Project**: Contenido de manifiestos partidarios
- **Cámara de Diputados de Chile**: Votaciones nominales (requiere scraping)

## Calidad de datos

No todos los datos son igualmente útiles. Evaluar calidad es responsabilidad del investigador.

### Dimensiones de calidad

**1. Validez**

¿Los datos miden lo que pretenden medir? Vimos validez de medición en Capítulo \@ref(medicion). Aquí, evaluar si las categorías y procedimientos de recolección capturan conceptos teóricos relevantes.

**2. Confiabilidad**

¿Los datos son consistentes? ¿Diferentes aplicaciones del mismo procedimiento producen resultados similares? En encuestas, verificar si preguntas se formulan igual en diferentes olas.

**3. Cobertura**

¿Qué población representan los datos? Si una encuesta solo contacta teléfonos fijos, subrepresenta jóvenes. Si datos administrativos solo cubren sector formal, omiten informalidad.

**4. Precisión**

¿Cuán exactas son las mediciones? Encuestas con muestras pequeñas tienen márgenes de error grandes. Datos administrativos pueden tener errores de digitación o categorización.

**5. Actualidad (timeliness)**

¿Cuán recientes son los datos? Para fenómenos que cambian rápido, datos de hace 5 años pueden ser obsoletos.

**6. Accesibilidad**

¿Los datos están disponibles? ¿En qué formato? ¿Con qué restricciones? Algunos datos excelentes son inaccesibles por razones legales o políticas.

### Problemas comunes

**Datos faltantes (missing data)**

Los valores faltantes son ubicuos. Pueden ser:

- **Completamente aleatorios (MCAR)**: La probabilidad de faltar no depende de nada. Raro en práctica.
- **Aleatorios condicional (MAR)**: La probabilidad depende de variables observadas. Manejable estadísticamente.
- **No aleatorios (MNAR)**: La probabilidad depende del valor no observado. Problemático.

Estrategias: eliminación listwise (perder casos), imputación, modelar explícitamente los datos faltantes.

**Errores de medición**

Discrepancias entre valor real y valor registrado. Pueden ser aleatorios (ruido) o sistemáticos (sesgo). Errores sistemáticos son más graves porque sesgan estimaciones.

**Codificación inconsistente**

Variables categóricas pueden codificarse de forma diferente en distintas partes del dataset. Ejemplo: género como "Masculino/Femenino", "M/F", "1/2", "Hombre/Mujer" en diferentes variables. Esto complica análisis.

**Valores atípicos (outliers)**

Observaciones extremas. Pueden ser errores de digitación o casos genuinos inusuales. Antes de eliminarlos, investigar.

**Duplicados**

Observaciones repetidas erróneamente. Común cuando múltiples fuentes se fusionan sin cuidado.

## Introducción a R y RStudio

R es un lenguaje de programación estadística y entorno para análisis de datos. RStudio es un entorno de desarrollo integrado (IDE) que hace más amigable trabajar con R.

### ¿Por qué R?

**Ventajas:**
- **Gratuito y de código abierto**: Sin costos de licencia
- **Reproducible**: Scripts documentan exactamente qué se hizo
- **Extensible**: Miles de paquetes para técnicas especializadas
- **Comunidad activa**: Foros, tutoriales, ayuda abundante
- **Estándar en ciencias sociales cuantitativas**: Facilita colaboración y replicación

**Desventajas:**
- **Curva de aprendizaje**: Programar requiere práctica
- **Múltiples formas de hacer lo mismo**: Puede ser confuso para principiantes

### Instalación

1. Descargar R: https://cran.r-project.org/
2. Descargar RStudio: https://posit.co/download/rstudio-desktop/
3. Instalar ambos (primero R, luego RStudio)

### Anatomía de RStudio

RStudio tiene cuatro paneles principales:

1. **Editor de scripts** (arriba izquierda): Donde escribes código para guardar y ejecutar
2. **Consola** (abajo izquierda): Donde se ejecuta el código y aparecen resultados
3. **Environment** (arriba derecha): Muestra objetos en memoria (datasets, variables)
4. **Files/Plots/Packages/Help** (abajo derecha): Navegación de archivos, gráficos, ayuda

### Paquetes

R base es poderoso, pero paquetes extienden funcionalidad. Instalación (una vez):

```{r eval=FALSE}
install.packages("tidyverse")  # Suite de paquetes para manipulación de datos
install.packages("haven")      # Leer archivos SPSS, Stata, SAS
install.packages("readxl")     # Leer archivos Excel
install.packages("foreign")    # Leer múltiples formatos
```

Cargar paquetes (cada sesión):

```{r eval=FALSE}
library(tidyverse)
library(haven)
```

### Objetos básicos en R

**Vectores**: Secuencias de elementos del mismo tipo

```{r}
edades <- c(23, 45, 67, 34, 29)
partidos <- c("PS", "UDI", "RN", "PC", "PPD")
```

**Data frames**: Tablas con filas (observaciones) y columnas (variables)

```{r}
datos <- data.frame(
  id = 1:5,
  edad = c(23, 45, 67, 34, 29),
  partido = c("PS", "UDI", "RN", "PC", "PPD"),
  voto = c("Apruebo", "Rechazo", "Rechazo", "Apruebo", "Apruebo")
)
```

**Funciones**: Operaciones que transforman inputs en outputs

```{r}
mean(edades)        # Promedio
sd(edades)          # Desviación estándar
table(datos$voto)   # Tabla de frecuencias
```

### Directorios de trabajo

R busca y guarda archivos en el "directorio de trabajo". Verificar:

```{r eval=FALSE}
getwd()  # ¿Dónde estoy?
```

Cambiar (ajustar ruta según tu computador):

```{r eval=FALSE}
setwd("~/Documentos/Investigacion/datos")
```

Mejor práctica: usar **proyectos de RStudio** (`.Rproj`). Esto establece automáticamente el directorio de trabajo en la carpeta del proyecto.

## Importar datos

Los datos pueden venir en múltiples formatos. R puede leer la mayoría.

### Archivos de texto: CSV

Los archivos CSV (comma-separated values) son universales. Formato simple:

```
id,edad,partido
1,23,PS
2,45,UDI
3,67,RN
```

Importar con R base:

```{r eval=FALSE}
datos <- read.csv("encuesta.csv")
```

Importar con `readr` (parte de tidyverse, más rápido):

```{r eval=FALSE}
library(readr)
datos <- read_csv("encuesta.csv")
```

**Parámetros comunes:**
- `sep`: Separador (`,` por defecto, `;` en algunos países)
- `header`: ¿Primera fila son nombres de variables? (TRUE por defecto)
- `na.strings`: Cómo se codifican valores faltantes ("NA", ".", "")

Ejemplo:

```{r eval=FALSE}
datos <- read_csv("encuesta.csv", 
                  na = c("", "NA", "No sabe"),
                  col_types = cols(edad = col_integer(),
                                   partido = col_character()))
```

### Archivos de Excel

Excel es ubicuo en administración pública. Paquete `readxl`:

```{r eval=FALSE}
library(readxl)
datos <- read_excel("resultados_electorales.xlsx", 
                    sheet = "Presidenciales_2021")
```

Especificar rango:

```{r eval=FALSE}
datos <- read_excel("archivo.xlsx", 
                    sheet = 2, 
                    range = "A1:F100")
```

### Archivos de software estadístico

**Stata (.dta):**

```{r eval=FALSE}
library(haven)
datos <- read_dta("encuesta_casen.dta")
```

**SPSS (.sav):**

```{r eval=FALSE}
datos <- read_sav("estudio_cep.sav")
```

**SAS:**

```{r eval=FALSE}
datos <- read_sas("datos.sas7bdat")
```

El paquete `haven` preserva etiquetas de valores, muy común en encuestas. Ejemplo:

```{r eval=FALSE}
datos <- read_dta("encuesta.dta")
# Variable "educ" tiene valores 1,2,3 con etiquetas "Primaria","Secundaria","Universitaria"
attributes(datos$educ)$labels
```

### Datos en línea

Muchos datos están en URLs. Importar directamente:

```{r eval=FALSE}
url <- "https://ejemplo.com/datos.csv"
datos <- read_csv(url)
```

Para datos de APIs, usar paquetes especializados:
- `WDI`: Banco Mundial
- `quantmod`: Datos financieros
- `rtweet`: API de Twitter (ahora X)

### Datos desde bases de datos

Para bases SQL, usar `DBI`:

```{r eval=FALSE}
library(DBI)
con <- dbConnect(RSQLite::SQLite(), "mi_base.db")
datos <- dbGetQuery(con, "SELECT * FROM votaciones WHERE year = 2021")
dbDisconnect(con)
```

## Manipulación básica de datos

Una vez importados, los datos rara vez están listos para análisis. Necesitan limpieza y transformación.

### Inspeccionar datos

Primeros y últimos casos:

```{r eval=FALSE}
head(datos)       # Primeros 6 casos
tail(datos, 10)   # Últimos 10 casos
```

Estructura:

```{r eval=FALSE}
str(datos)        # Tipo de cada variable
summary(datos)    # Resumen estadístico
glimpse(datos)    # Vista compacta (tidyverse)
```

Nombres de variables:

```{r eval=FALSE}
names(datos)
colnames(datos)
```

Dimensiones:

```{r eval=FALSE}
dim(datos)        # Filas y columnas
nrow(datos)       # Número de filas
ncol(datos)       # Número de columnas
```

### Seleccionar variables

R base:

```{r eval=FALSE}
datos_reducido <- datos[, c("id", "edad", "partido")]
```

Tidyverse (`dplyr`):

```{r eval=FALSE}
library(dplyr)
datos_reducido <- select(datos, id, edad, partido)
```

Seleccionar por criterio:

```{r eval=FALSE}
# Todas las variables que empiezan con "voto_"
datos_reducido <- select(datos, starts_with("voto_"))

# Todas las variables numéricas
datos_reducido <- select(datos, where(is.numeric))
```

### Filtrar casos

R base:

```{r eval=FALSE}
jovenes <- datos[datos$edad < 30, ]
```

Tidyverse:

```{r eval=FALSE}
jovenes <- filter(datos, edad < 30)
jovenes_izquierda <- filter(datos, edad < 30 & partido %in% c("PS", "PC", "FA"))
```

### Crear y transformar variables

R base:

```{r eval=FALSE}
datos$edad_decadas <- datos$edad / 10
datos$mayor_edad <- ifelse(datos$edad >= 18, "Sí", "No")
```

Tidyverse:

```{r eval=FALSE}
datos <- mutate(datos, 
                edad_decadas = edad / 10,
                mayor_edad = if_else(edad >= 18, "Sí", "No"),
                edad_cat = case_when(
                  edad < 30 ~ "Joven",
                  edad < 60 ~ "Adulto",
                  TRUE ~ "Mayor"
                ))
```

### Recodificar variables

Cambiar categorías:

```{r eval=FALSE}
datos <- mutate(datos,
                partido_bloques = case_when(
                  partido %in% c("PS", "PC", "PPD", "FA") ~ "Izquierda",
                  partido %in% c("UDI", "RN", "EVOPOLI") ~ "Derecha",
                  TRUE ~ "Centro/Otros"
                ))
```

### Renombrar variables

R base:

```{r eval=FALSE}
names(datos)[names(datos) == "p1"] <- "confianza_congreso"
```

Tidyverse:

```{r eval=FALSE}
datos <- rename(datos, 
                confianza_congreso = p1,
                confianza_gobierno = p2)
```

### Ordenar datos

Por edad (ascendente):

```{r eval=FALSE}
datos_ordenado <- arrange(datos, edad)
```

Por edad (descendente):

```{r eval=FALSE}
datos_ordenado <- arrange(datos, desc(edad))
```

Por múltiples variables:

```{r eval=FALSE}
datos_ordenado <- arrange(datos, partido, edad)
```

### Agregar datos

Resumir por grupos:

```{r eval=FALSE}
library(dplyr)
resumen_partido <- datos %>%
  group_by(partido) %>%
  summarise(
    n = n(),
    edad_promedio = mean(edad, na.rm = TRUE),
    edad_sd = sd(edad, na.rm = TRUE)
  )
```

El operador `%>%` (pipe) pasa el resultado de una función a la siguiente. El código anterior dice: "toma `datos`, agrúpalo por `partido`, calcula estadísticos".

### Unir datasets

**Left join**: Mantener todos los casos del dataset izquierdo

```{r eval=FALSE}
datos_completo <- left_join(encuesta, resultados_electorales, 
                             by = "comuna_id")
```

**Inner join**: Mantener solo casos presentes en ambos datasets

```{r eval=FALSE}
datos_completo <- inner_join(encuesta, resultados_electorales, 
                              by = "comuna_id")
```

## Exportar datos

Después de limpiar y transformar datos, es útil guardarlos.

### CSV

```{r eval=FALSE}
write_csv(datos_limpios, "datos_procesados.csv")
```

### Formato de R

Guardar como `.RDS` (R Data Serialization) preserva estructura completa:

```{r eval=FALSE}
saveRDS(datos_limpios, "datos_procesados.rds")
```

Leer después:

```{r eval=FALSE}
datos <- readRDS("datos_procesados.rds")
```

### Stata

```{r eval=FALSE}
library(haven)
write_dta(datos_limpios, "datos_procesados.dta")
```

### Excel

```{r eval=FALSE}
library(writexl)
write_xlsx(datos_limpios, "datos_procesados.xlsx")
```

## Flujo de trabajo reproducible

La reproducibilidad es esencial en ciencia. Otros (y tu yo futuro) deben poder replicar exactamente lo que hiciste.

### Principios

**1. Usa scripts, no clics**

Todo debe estar en código. No hacer transformaciones manualmente en Excel que no queden documentadas.

**2. Nunca modifiques datos originales**

Lee los datos crudos, transfórmalos en R, guarda la versión procesada. Mantén los originales intactos.

**3. Documenta tu código**

Comentarios explican *por qué* hiciste algo:

```{r eval=FALSE}
# Recodifico educación en tres categorías para simplificar análisis
# La categoría "técnica" es pequeña y la combino con "secundaria"
datos <- mutate(datos,
                educ_cat = case_when(
                  educ <= 2 ~ "Básica",
                  educ %in% c(3,4) ~ "Media",
                  educ >= 5 ~ "Superior"
                ))
```

**4. Organiza tu proyecto**

Estructura de carpetas clara:

```
proyecto/
  datos/
    crudos/
    procesados/
  scripts/
    01_limpieza.R
    02_analisis.R
  resultados/
    tablas/
    graficos/
  documento/
```

**5. Usa control de versiones**

Git y GitHub permiten rastrear cambios y colaborar. No es obligatorio para principiantes, pero altamente recomendable a medida que proyectos crecen.

### R Markdown

R Markdown integra código, resultados y texto en un solo documento. Permite generar reportes reproducibles en HTML, PDF o Word.

Ejemplo básico (`analisis.Rmd`):

```
---
title: "Análisis de participación electoral"
author: "Tu Nombre"
date: "`r Sys.Date()`"
output: html_document
---

## Introducción

Este análisis examina factores asociados a participación electoral.

```{r setup-04, include=FALSE, eval=FALSE}
library(tidyverse)
datos <- read_csv("datos/encuesta.csv")
```

## Resultados

La participación promedio fue XX%.

```{r grafico, echo=FALSE, fig.cap="Participación por edad", eval=FALSE}
ggplot(datos, aes(x = edad, y = participo)) +
  geom_smooth() +
  theme_minimal()
```
```

Al compilar ("Knit"), R ejecuta el código e inserta resultados en el documento final. Si los datos cambian, el documento se actualiza automáticamente.

## Resumen del capítulo

Los datos de ciencias políticas varían en estructura (corte transversal, series temporales, panel, jerárquicos), granularidad (micro, meso, macro) y fuente (primarios, secundarios, administrativos, digitales). Cada tipo tiene ventajas y limitaciones.

Evaluar calidad de datos es crucial: validez, confiabilidad, cobertura, precisión, actualidad, accesibilidad. Problemas comunes incluyen datos faltantes, errores de medición, codificación inconsistente, outliers y duplicados.

R y RStudio proveen un entorno poderoso y reproducible para análisis cuantitativo. Los datos pueden importarse desde múltiples formatos (CSV, Excel, Stata, SPSS, bases de datos, URLs). Los paquetes `readr`, `haven`, `readxl` y `foreign` facilitan importación.

La manipulación de datos involucra inspeccionar, seleccionar variables, filtrar casos, crear nuevas variables, recodificar, ordenar, agregar y unir datasets. El paquete `dplyr` (parte de tidyverse) provee funciones intuitivas para estas operaciones.

Exportar datos procesados en formatos apropiados preserva trabajo y facilita colaboración. El flujo de trabajo reproducible —usando scripts, documentando código, manteniendo organización clara, nunca modificando datos originales— es esencial para investigación rigurosa.

## Lecturas recomendadas

**Manipulación de datos en R:**

Wickham, H., & Grolemund, G. (2017). *R for Data Science*. O'Reilly. [Disponible gratis en https://r4ds.had.co.nz/]  
→ Guía comprehensiva y accesible sobre tidyverse.

**Reproducibilidad:**

Gandrud, C. (2015). *Reproducible Research with R and RStudio* (2nd ed.). CRC Press.  
→ Prácticas y herramientas para investigación reproducible.

**Calidad de datos:**

Karr, A. F., Sanil, A. P., & Banks, D. L. (2006). Data quality: A statistical perspective. *Statistical Methodology*, 3(2), 137-173.  
→ Perspectiva estadística sobre dimensiones de calidad de datos.

**Datos faltantes:**

Little, R. J., & Rubin, D. B. (2019). *Statistical Analysis with Missing Data* (3rd ed.). Wiley.  
→ Tratado técnico sobre manejo de datos faltantes.

**Recursos en línea:**

- Documentación de tidyverse: https://www.tidyverse.org/
- RStudio cheatsheets: https://posit.co/resources/cheatsheets/
- Stack Overflow (para preguntas específicas): https://stackoverflow.com/questions/tagged/r

## Ejercicios

**1. Evaluación de calidad**

Descarga datos del CEP (https://www.cepchile.cl/opinion-publica/encuesta-cep/) o Latinobarómetro (https://www.latinobarometro.org/). 

a) ¿Qué población representa la muestra?  
b) ¿Cuál es el tamaño muestral y margen de error?  
c) Identifica tres variables. Para cada una, evalúa: ¿Es válida para medir el concepto que pretende? ¿Qué problemas de medición podrían existir?  
d) ¿Qué información sobre calidad de datos provee la documentación?  

**2. Importar y explorar**

Importa un dataset de tu elección en R:

```{r eval=FALSE}
# Tu código aquí
```

a) ¿Cuántos casos y variables tiene?  
b) ¿Qué tipos de variables contiene (numéricas, categóricas)?  
c) ¿Cuántos valores faltantes hay en cada variable?  
d) Identifica y describe un valor atípico  

**3. Limpieza de datos**

Con el dataset del ejercicio 2:

a) Selecciona 5-10 variables relevantes para una pregunta de investigación que formules  
b) Filtra casos para incluir solo observaciones completas (sin valores faltantes en variables clave)  
c) Crea al menos dos variables nuevas mediante transformación o recodificación  
d) Genera un resumen estadístico de tus variables  

**4. Datos faltantes**

```{r eval=FALSE}
# Crea un dataset con valores faltantes
set.seed(123)
datos <- data.frame(
  id = 1:100,
  edad = sample(18:80, 100, replace = TRUE),
  ingreso = rnorm(100, 500000, 150000)
)
# Introduce valores faltantes no aleatorios:
# Personas mayores de 60 tienen 50% probabilidad de no reportar ingreso
datos$ingreso[datos$edad > 60 & runif(100) < 0.5] <- NA
```

a) ¿Cuántos valores faltantes hay en `ingreso`?  
b) Calcula ingreso promedio: (1) eliminando casos con valores faltantes, (2) solo entre quienes reportaron. ¿Difieren las estimaciones? ¿Por qué?  
c) ¿Qué tipo de datos faltantes son estos (MCAR, MAR, MNAR)?  
d) ¿Cómo afecta esto las inferencias sobre ingreso promedio en la población?  

**5. Combinar datasets**

Tienes dos datasets:

```{r eval=FALSE}
# Resultados electorales por comuna
resultados <- data.frame(
  comuna_id = 1:5,
  comuna = c("Santiago", "Valparaíso", "Concepción", "La Serena", "Temuco"),
  votos_derecha = c(45, 38, 42, 50, 35),
  votos_izquierda = c(40, 48, 43, 35, 50)
)

# Características socioeconómicas
socioeconomico <- data.frame(
  comuna_id = c(1, 2, 3, 6),
  ingreso_promedio = c(800000, 650000, 700000, 600000),
  desigualdad_gini = c(0.48, 0.45, 0.47, 0.50)
)
```

a) Combina los datasets manteniendo todas las comunas de `resultados`  
b) ¿Qué problema de datos observas después de combinar?  
c) ¿Cómo manejarías ese problema para análisis posterior?  

**6. Proyecto propio**

Identifica un dataset relevante para tu tesis o interés de investigación:

a) Descríbelo: fuente, estructura (corte transversal/panel/etc), unidad de análisis, N  
b) Importa a R y documenta el proceso  
c) Realiza una limpieza básica: seleccionar variables relevantes, verificar valores faltantes y outliers, crear variables derivadas si es necesario  
d) Genera un reporte en R Markdown que documente: tu pregunta de investigación, descripción de los datos, proceso de limpieza, y estadísticas descriptivas básicas  
e) Exporta la versión limpia de los datos
