# Regresión bivariada {#regresion-bivariada}

## Objetivos del capítulo

Al finalizar este capítulo, deberías ser capaz de:

- Calcular e interpretar coeficientes de correlación
- Comprender la diferencia entre correlación y causalidad
- Estimar modelos de regresión lineal simple usando mínimos cuadrados ordinarios
- Interpretar coeficientes de regresión e intercepto
- Evaluar la bondad de ajuste usando $R^2$
- Realizar pruebas de hipótesis sobre coeficientes de regresión

## Correlación

La **correlación** mide la fuerza y dirección de la relación lineal entre dos variables.

**Coeficiente de correlación de Pearson:**
$$r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2} \sqrt{\sum (y_i - \bar{y})^2}}$$

- $-1 \leq r \leq 1$
- $r = 1$: Correlación positiva perfecta
- $r = 0$: Sin correlación lineal
- $r = -1$: Correlación negativa perfecta

::: {.ejemplo}
**Caso: Desarrollo económico y democracia**

```{r correlacion-ejemplo, echo=TRUE}
# Datos simulados: PIB per cápita y nivel de democracia
set.seed(2024)
n <- 50
pib <- rnorm(n, mean = 15000, sd = 8000)
democracia <- 3 + 0.0003 * pib + rnorm(n, mean = 0, sd = 1.5)

# Correlación
cor_pearson <- cor(pib, democracia)
cat("Correlación de Pearson:", round(cor_pearson, 3), "\n")

# Test de significancia
cor.test(pib, democracia)
```
:::

::: {.importante}
**Correlación ≠ Causalidad**

Una correlación fuerte NO implica que una variable cause la otra. Pueden existir:
- Variables confusoras
- Causalidad inversa
- Relaciones espurias
:::

## El modelo de regresión lineal simple

La regresión lineal modela la relación entre una variable dependiente $Y$ y una variable independiente $X$:

$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$

donde:
- $\beta_0$ = intercepto (valor de $Y$ cuando $X = 0$)
- $\beta_1$ = pendiente (cambio en $Y$ por unidad de cambio en $X$)
- $\epsilon_i$ = error (variación no explicada por el modelo)

## Mínimos cuadrados ordinarios

El método de **mínimos cuadrados ordinarios** (OLS, *Ordinary Least Squares*) estima $\beta_0$ y $\beta_1$ minimizando la suma de errores al cuadrado:

$$\min_{\beta_0, \beta_1} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$$

**Fórmulas OLS:**

$$\hat{\beta}_1 = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2} = r \cdot \frac{s_Y}{s_X}$$

$$\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}$$

::: {.ejemplo}
**Caso: Gasto público y crecimiento económico**

```{r regresion-ejemplo, echo=TRUE}
# Datos simulados
set.seed(456)
gasto_publico <- runif(40, min = 15, max = 40)  # % del PIB
crecimiento <- 5 - 0.15 * gasto_publico + rnorm(40, mean = 0, sd = 1.2)

# Modelo de regresión
modelo <- lm(crecimiento ~ gasto_publico)
summary(modelo)
```

```{r plot-regresion, echo=FALSE, fig.cap="Regresión lineal: Gasto público y crecimiento"}
library(ggplot2)

datos <- data.frame(gasto_publico, crecimiento)

ggplot(datos, aes(x = gasto_publico, y = crecimiento)) +
  geom_point(size = 3, alpha = 0.6, color = "#3498db") +
  geom_smooth(method = "lm", se = TRUE, color = "#e74c3c", fill = "#e74c3c", alpha = 0.2) +
  labs(
    title = "Relación entre gasto público y crecimiento económico",
    x = "Gasto público (% del PIB)",
    y = "Crecimiento económico (%)"
  ) +
  theme_minimal()
```
:::

## Interpretación de coeficientes

**Intercepto ($\hat{\beta}_0$)**: Valor predicho de $Y$ cuando $X = 0$

**Pendiente ($\hat{\beta}_1$)**: Cambio promedio en $Y$ asociado con un incremento de una unidad en $X$

::: {.ejemplo}
Si $\hat{\beta}_1 = -0.15$ en el modelo anterior:

"Por cada punto porcentual adicional de gasto público, el crecimiento económico disminuye en promedio 0.15 puntos porcentuales."
:::

## Bondad de ajuste

### R cuadrado ($R^2$)

El **coeficiente de determinación** mide qué proporción de la variabilidad en $Y$ es explicada por $X$:

$$R^2 = 1 - \frac{\sum (Y_i - \hat{Y}_i)^2}{\sum (Y_i - \bar{Y})^2}$$

- $0 \leq R^2 \leq 1$
- $R^2 = 0$: El modelo no explica nada
- $R^2 = 1$: El modelo explica perfectamente

::: {.ejemplo}
Si $R^2 = 0.42$:

"El 42% de la variabilidad en el crecimiento económico es explicada por el gasto público. El 58% restante se debe a otros factores."
:::

::: {.importante}
**$R^2$ elevado ≠ Causalidad**

Un $R^2$ alto NO implica relación causal. Puede haber:
- Variables omitidas importantes
- Relación espuria
- Causalidad inversa
:::

## Resumen del capítulo

**Conceptos clave:**

1. **Correlación**: Mide fuerza y dirección de relación lineal (no implica causalidad)
2. **Regresión lineal**: Modela $Y$ como función lineal de $X$
3. **OLS**: Método para estimar coeficientes minimizando errores al cuadrado
4. **Interpretación**: $\beta_0$ = intercepto, $\beta_1$ = pendiente
5. **$R^2$**: Proporción de variabilidad explicada por el modelo

**Fórmulas clave:**

- Correlación: $r = \frac{\text{Cov}(X,Y)}{s_X s_Y}$
- Pendiente: $\hat{\beta}_1 = r \cdot \frac{s_Y}{s_X}$
- Intercepto: $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}$
- Bondad de ajuste: $R^2 = 1 - \frac{\text{SSR}}{\text{SST}}$

## Lecturas recomendadas

- Agresti, A. & Finlay, B. (2009). *Statistical Methods for the Social Sciences*. Capítulo 9: Linear Regression and Correlation.
- Llaudet, E. & Imai, K. (2022). *Data Analysis for Social Science*. Capítulo 4: Prediction.

## Ejercicios

1. Calcula la correlación entre años de educación e ingreso. Interpreta el resultado.

2. Estima un modelo de regresión para predecir participación electoral a partir de nivel educacional promedio por comuna.

3. Interpreta los coeficientes y el $R^2$ del modelo estimado en el ejercicio 2.

4. Discute por qué una correlación alta entre helado y ahogamientos NO implica causalidad.
